library(ggplot2)
library(lattice)
library(caret)
library(e1071)
library(Hmisc)
library(survival)
library(dplyr)
#install.packages("mltools")
library(mltools)
#install.packages("C50")
library(C50)
# Read the data
D<- read.csv(file = 'C:/Users/marwa/OneDrive/Desktop/machinelearning/Telco_customer_churn.csv')
#Cleaning the data by removing unique columns
Data<- select(D,-c(1,2,29))
atr <- attributes(Data)

#Pre-processing the dataset to convert all the features to numeric value
for (i in 1:(ncol(Data)-1)) {
  if (is.character(Data[, i])==TRUE){
    for(j in 1:nrow(Data)){
      ascis <- as.numeric(charToRaw(Data[j, i]))
      Data[ j, i] <- sum(ascis)
    }
  }
  Data[,i] <- as.numeric(Data[,i])
}
#converting target column to factor
Data[,ncol(Data)] <-(as.factor(Data[,ncol(Data)]))
#omitting empty rows only 0.15% of data is missing values
newData<-na.omit(Data)
Grid = data.frame(usekernel=TRUE,laplace = 0,adjust=1)
newData<-na.omit(Data)
mdl = train(Churn.Value~ .,data=newData,method="naive_bayes",
            trControl=trainControl(method="none"),
            tuneGrid=Grid)

varImp(mdl)
#options(max.print=9999999)
#rstudioapi::writeRStudioPreference("console_max_lines", 10000)

#is.na(Data)

summary(newData)
FeatureSelectedData<-subset(newData, select = c(Churn.Score,Tenure.Months,Contract, Online.Security, Tech.Support, Internet.Service, Online.Backup,Device.Protection,Total.Charges,Streaming.Movies,Monthly.Charges,Streaming.TV,Dependents,Paperless.Billing,Partner,CLTV,Payment.Method,Senior.Citizen, Churn.Value))

#5.Find the percentages of rows that belong to the labels. HINT: Use prop.table( )and table( ).
prop.table(table(newData$Churn.Value)) * 100
 #6.Find the coherent numerical summary of dataset. HINT: Use describe( ).
describe(newData)

# *********** D.Step 4 – Classification model creation, training, testing, andperformance evaluation ***********
#****************************************************************************************************************
# 7. Create C5.0 decision tree classification model and make predictions without using data normalisation (below code)
# header line of on-screen performance metrics
message("\t Folds \t Accuracy% \t Kappa% \t Sensitivity% \t Specificity% \tPrecision% \t Recall%")

# creating a blank data frame to store performance metrics scores
pf = data.frame(matrix(vector(), 5, 6, dimnames=list(c("Fold-1", "Fold-2", "Fold-3", "Fold-4", "Fold-5"),
                                c("Accuracy","Kappa","Sensitivity","Specificity","Precision","Recall"))),
  stringsAsFactors=F)
pfc <- 0
FD <- 5
Folds <- createFolds(FeatureSelectedData$Churn.Value, k = FD, list = TRUE, returnTrain = TRUE)
for (i in 1:FD){
  pfc <- pfc+1
  Held_Out_Indices = Folds[[i]]
  Training_Set = FeatureSelectedData[Held_Out_Indices,]
  Testing_Set = FeatureSelectedData[-Held_Out_Indices,]
  TrainedClassifier <- C5.0(Churn.Value~., data = Training_Set) # for C5.0 model creation
  # ***********************************************************************
  # *********** E. Step 5 – Performance improvement, Exercise-9 ***********
  # ***********************************************************************
  # Uncomment Line 122 for Exercise-9 & comment Line 115
  #TrainedClassifier <- C5.0(y ~ ., data = Training_Set, trials = 3) # C5.0 method with trials argument, try with different values of trials
  # ************************************************************************
  # *********** E. Step 5 – Performance improvement, Exercise-10 ***********
  # ************************************************************************
  # Uncomment Line 130 to 132 for Exercise-10 & comment Line 115
  #A cost-matrix can also be used to emphasize certain classes over others. For example, to get more of the “no” samples correct:
  #cost_mat <- matrix(c(0, 2, 1, 0), nrow = 2)
  #rownames(cost_mat) <- colnames(cost_mat) <- c("no", "yes")
  #TrainedClassifier <- C5.0(y ~ ., data = Training_Set, costs=cost_mat) 
  # C5.0 method with cost matrix argument, an example is given but you can create your own cost matrix
  #To see the tree's decisions, we can call the summary() function on the model:
  summary(TrainedClassifier)
  #We can see some basic data about the tree by typing its name:
  TrainedClassifier
  #A graphical method for examining the model can be generated by the plot method:
  plot(TrainedClassifier)
  Predictions <- predict(TrainedClassifier, newdata=Testing_Set)
  cm <- confusionMatrix(Testing_Set$Churn.Value, Predictions)
  cm
  # You can access performance metrics from here
  # below message() function shows the performance metrics on-screen
  message("\tFold-",i, "\t ", format(round(cm[["overall"]][["Accuracy"]]*100, 2),
                                     nsmall = 2), "\t\t ",
          format(round(cm[["overall"]][["Kappa"]]*100, 2), nsmall = 2), "\t\t ",
          format(round(cm[["byClass"]][["Sensitivity"]]*100, 2), nsmall = 2), "\t\t ",
          format(round(cm[["byClass"]][["Specificity"]]*100, 2), nsmall = 2), "\t\t ",
          format(round(cm[["byClass"]][["Precision"]]*100, 2), nsmall = 2), "\t\t ",
          format(round(cm[["byClass"]][["Recall"]]*100, 2), nsmall = 2))
  # --------- assigning the performance metrics to the dataframe created----------------
  pf[pfc,"Accuracy"] <- format(round(cm[["overall"]][["Accuracy"]]*100, 2), nsmall =
                                   2)
  pf[pfc,"Kappa"] <- format(round(cm[["overall"]][["Kappa"]]*100, 2), nsmall = 2)
  pf[pfc,"Sensitivity"] <- format(round(cm[["byClass"]][["Sensitivity"]]*100, 2),
                                  nsmall = 2)
  pf[pfc,"Specificity"] <- format(round(cm[["byClass"]][["Specificity"]]*100, 2),
                                  nsmall = 2)
  pf[pfc,"Precision"] <- format(round(cm[["byClass"]][["Precision"]]*100, 2), nsmall
                                = 2)
  pf[pfc,"Recall"] <- format(round(cm[["byClass"]][["Recall"]]*100, 2), nsmall = 2)
}
message("-----------------------------------------------------------------------------
----------------------------------------------------")
# after all the itarations for varying training and testing data amount, reading and bar-plotting each metric column from the pf dataframe
par(mfrow=c(3,2))
barplot(t(as.matrix(pf["Accuracy"])), main="Accuracy", xlab="Training Data (%)",
        ylab="Accuracy (%)", names.arg = c(row.names(pf)), col = rainbow(20),ylim=c(0,100))
barplot(t(as.matrix(pf["Kappa"])), main="Kappa", xlab="Training Data (%)",
        ylab="Kappa (%)", names.arg = c(row.names(pf)), col = rainbow(20),ylim=c(0,100))
barplot(t(as.matrix(pf["Sensitivity"])), main="Sensitivity", xlab="Training Data
(%)", ylab="Sensitivity (%)", names.arg = c(row.names(pf)), col =
          rainbow(20),ylim=c(0,100))
barplot(t(as.matrix(pf["Specificity"])), main="Specificity", xlab="Training Data
(%)", ylab="Specificity (%)", names.arg = c(row.names(pf)), col =
          rainbow(20),ylim=c(0,100))
barplot(t(as.matrix(pf["Precision"])), main="Precision", xlab="Training Data (%)",
        ylab="Precision (%)", names.arg = c(row.names(pf)), col = rainbow(20),ylim=c(0,100))
barplot(t(as.matrix(pf["Recall"])), main="Recall", xlab="Training Data (%)",
        ylab="Recall (%)", names.arg = c(row.names(pf)), col = rainbow(20),ylim=c(0,100))